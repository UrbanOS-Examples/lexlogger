global:
  ingress:
    annotations:
      kubernetes.io/ingress.class: alb
      alb.ingress.kubernetes.io/tags: scos.delete.on.teardown=true
      alb.ingress.kubernetes.io/scheme: internal

rbac:
  create: true

serviceAccounts:
  fluentbit:
    create: true
    name:
  kibana:
    create: true
    name:

elasticsearch:
  image:
    repository: "docker.elastic.co/elasticsearch/elasticsearch-oss"
    tag: "6.4.2"
    pullPolicy: "IfNotPresent"

  initImage:
    repository: "busybox"
    tag: "latest"
    pullPolicy: "Always"

  cluster:
    name: "es-lexlogger"
    # If you want X-Pack installed, switch to an image that includes it, enable this option and toggle the features you want
    # enabled in the environment variables outlined in the README
    xpackEnable: false
    # Some settings must be placed in a keystore, so they need to be mounted in from a secret.
    # Use this setting to specify the name of the secret
    # keystoreSecret: eskeystore
    config: {}
    # Custom parameters, as string, to be added to ES_JAVA_OPTS environment variable
    additionalJavaOpts: ""
    env:
      # IMPORTANT: https://www.elastic.co/guide/en/elasticsearch/reference/current/important-settings.html#minimum_master_nodes
      # To prevent data loss, it is vital to configure the discovery.zen.minimum_master_nodes setting so that each master-eligible
      # node knows the minimum number of master-eligible nodes that must be visible in order to form a cluster.
      MINIMUM_MASTER_NODES: "2"

  client:
    name: es-client
    replicas: 1
    serviceType: ClusterIP
    loadBalancerIP: {}
    loadBalancerSourceRanges: {}
  ## (dict) If specified, apply these annotations to the client service
  #  serviceAnnotations:
  #    example: client-svc-foo
    heapSize: "512m"
    antiAffinity: "soft"
    nodeAffinity: {}
    nodeSelector: {}
    tolerations: []
    resources:
      limits:
        cpu: "250m"
        memory: "1024Mi"
      requests:
        cpu: "100m"
        memory: "1024Mi"
    priorityClassName: ""
    ## (dict) If specified, apply these annotations to each client Pod
    # podAnnotations:
    #   example: client-foo
    podDisruptionBudget:
      enabled: false
      minAvailable: 1
      # maxUnavailable: 1

  master:
    name: es-master
    exposeHttp: false
    replicas: 2
    heapSize: "512m"
    persistence:
      enabled: true
      accessMode: ReadWriteOnce
      name: data
      size: "4Gi"
    antiAffinity: "soft"
    nodeAffinity: {}
    nodeSelector: {}
    tolerations: []
    resources:
      limits:
        cpu: "250m"
        memory: "1024Mi"
      requests:
        cpu: "100m"
        memory: "1024Mi"
    priorityClassName: ""
    ## (dict) If specified, apply these annotations to each master Pod
    # podAnnotations:
    #   example: master-foo
    podDisruptionBudget:
      enabled: false
      minAvailable: 1  # Same as `cluster.env.MINIMUM_MASTER_NODES`
      # maxUnavailable: 1
    updateStrategy:
      type: OnDelete

  data:
    name: es-data
    exposeHttp: false
    replicas: 2
    heapSize: "1536m"
    persistence:
      enabled: true
      accessMode: ReadWriteOnce
      name: data
      size: "30Gi"
    terminationGracePeriodSeconds: 3600
    antiAffinity: "soft"
    nodeAffinity: {}
    nodeSelector: {}
    tolerations: []
    resources:
      limits:
        cpu: "250m"
        memory: "2048Mi"
      requests:
        cpu: "100m"
        memory: "2048Mi"
    priorityClassName: ""
    ## (dict) If specified, apply these annotations to each data Pod
    # podAnnotations:
    #   example: data-foo
    podDisruptionBudget:
      enabled: false
      # minAvailable: 1
      maxUnavailable: 1
    updateStrategy:
      type: OnDelete

  ## Additional init containers
  extraInitContainers: |

fluentbit:
  name: fluentbit
  on_minikube: false

  image:
    repository: fluent/fluent-bit
    tag: 0.14.4
    pullPolicy: Always

  # When enabled, exposes json and prometheus metrics on {{ .Release.Name }}-metrics service
  metrics:
    enabled: false
    service:
      port: 2020
      type: ClusterIP

  # When enabled, fluent-bit will keep track of tailing offsets across pod restarts.
  trackOffsets: false

  parsers:
    enabled: false
    ## List the respective parsers in key: value format per entry
    ## Regex required fields are name and regex. JSON required field
    ## is name.
    regex: []
    json: []

  env: []
  podAnnotations: {}

  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.existingConfigMap}}
  ## Defining existingConfigMap will cause templates/config.yaml
  ## to NOT generate a ConfigMap resource
  ##
  existingConfigMap: ""

  ## Extra volumes containing additional files required for fluent-bit to work
  ## (eg. CA certificates)
  ## Ref: https://kubernetes.io/docs/concepts/storage/volumes/
  extraVolumes: []

  ## Extra volume mounts for the fluent-bit pod.
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/
  extraVolumeMounts: []

  resources:
    limits:
      memory: 100Mi
    requests:
      cpu: 100m
      memory: 100Mi

  tolerations: []

  nodeSelector: {}

  filter:
    kubeURL: https://kubernetes.default.svc:443
    kubeCAFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    kubeTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
    kubeTag: kube
  # If true, check to see if the log field content is a JSON string map, if so,
  # it append the map fields as part of the log structure.
  #  mergeJSONLog: true

  # If true, enable the use of monitoring for a pod annotation of
  # fluentbit.io/parser: parser_name. parser_name must be the name
  # of a parser contained within parsers.conf
  #  enableParser: true

  # If true, enable the use of monitoring for a pod annotation of
  # fluentbit.io/exclude: true. If present, discard logs from that pod.
  #  enableExclude: true

fluentd:
  name: fluentd

  image:
    repository: gcr.io/google-containers/fluentd-elasticsearch
    tag: v2.3.1
    pullPolicy: IfNotPresent

  output:
    buffer_chunk_limit: 2M
    buffer_queue_limit: 8

  env: {}

  resources:
    limits:
      memory: 200Mi
    requests:
      cpu: 100m
      memory: 200Mi

  service:
    type: ClusterIP
    externalPort: 80
    ports:
      - name: "monitor-agent"
        protocol: TCP
        containerPort: 24220
      - name: "fbit-forward"
        protocol: TCP
        containerPort: 24224

  ingress:
    enabled: false
    # hosts:
    #   - name: "http-input.local"
    #     protocol: TCP
    #     serviceName: http-input
    #     servicePort: 9880
    annotations:
      alb.ingress.kubernetes.io/healthcheck-path: /api/plugins.json
      alb.ingress.kubernetes.io/healthcheck-port: "24220"
    tls:

  configMaps:
    general.conf: |
      # Prevent fluentd from handling records containing its own logs. Otherwise
      # it can lead to an infinite loop, when error in sending one message generates
      # another message which also fails to be sent and so on.
      <match fluentd.**>
        @type null
      </match>
      # Used for health checking
      <source>
        @type http
        port 9880
        bind 0.0.0.0
      </source>
      # Emits internal metrics to every minute, and also exposes them on port
      # 24220. Useful for determining if an output plugin is retryring/erroring,
      # or determining the buffer queue length.
      <source>
        @type monitor_agent
        bind 0.0.0.0
        port 24220
        tag fluentd.monitor.metrics
      </source>
    system.conf: |-
      <system>
        root_dir /tmp/fluentd-buffers/
      </system>
    forward-input.conf: |
      <source>
        @type forward
        port 24224
        bind 0.0.0.0
      </source>
    output.conf: |
      <match **>
        @id elasticsearch
        @type elasticsearch
        @log_level info
        include_tag_key true
        # Replace with the host/port to your Elasticsearch cluster.
        host "#{ENV['OUTPUT_HOST']}"
        port "#{ENV['OUTPUT_PORT']}"
        logstash_format true
        <buffer>
          @type file
          path /var/log/fluentd-buffers/kubernetes.system.buffer
          flush_mode interval
          retry_type exponential_backoff
          flush_thread_count 2
          flush_interval 5s
          retry_forever
          retry_max_interval 30
          chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
          queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
          overflow_action block
        </buffer>
      </match>

  ## Persist data to a persistent volume
  persistence:
    enabled: false

    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    # storageClass: "-"
    # annotations: {}
    accessMode: ReadWriteOnce
    size: 10Gi
  nodeSelector: {}
  tolerations: []
  affinity: {}

kibana:
  name: kibana
  
  image:
    repository: "docker.elastic.co/kibana/kibana-oss"
    tag: "6.4.2"
    pullPolicy: "IfNotPresent"

  commandline:
    args:

  env: {}
    # All Kibana configuration options are adjustable via env vars.
    # To adjust a config option to an env var uppercase + replace `.` with `_`
    # Ref: https://www.elastic.co/guide/en/kibana/current/settings.html
    #
    # ELASTICSEARCH_URL: http://elasticsearch-client:9200
    # SERVER_PORT: 5601
    # LOGGING_VERBOSE: "true"
    # SERVER_DEFAULTROUTE: "/app/kibana"

  files:
    kibana.yml:
      ## Default Kibana configuration from kibana-docker.
      server.name: kibana
      server.host: "0"
      elasticsearch.url: http://elasticsearch:9200

      ## Custom config properties below
      ## Ref: https://www.elastic.co/guide/en/kibana/current/settings.html
      # server.port: 5601
      # logging.verbose: "true"
      # server.defaultRoute: "/app/kibana"

  service:
    type: NodePort
    externalPort: 443
    internalPort: 5601
    # authProxyPort: 5602 To be used with authProxyEnabled and a proxy extraContainer
    ## External IP addresses of service
    ## Default: nil
    ##
    # externalIPs:
    # - 192.168.0.1
    #
    ## LoadBalancer IP if service.type is LoadBalancer
    ## Default: nil
    ##
    # loadBalancerIP: 10.2.2.2
    annotations:
    labels:

  ingress:
    enabled: true
    # hosts:
      # - chart-example.local
    annotations:
      alb.ingress.kubernetes.io/healthcheck-path: /status
    #   kubernetes.io/tls-acme: "true"
    # tls:
      # - secretName: chart-example-tls
      #   hosts:
      #     - chart-example.local

  livenessProbe:
    enabled: false
    initialDelaySeconds: 30
    timeoutSeconds: 10

  readinessProbe:
    enabled: false
    initialDelaySeconds: 30
    timeoutSeconds: 10

  # Enable an authproxy. Specify container in extraContainers
  authProxyEnabled: false

  extraContainers: |
  # - name: proxy
  #   image: quay.io/gambol99/keycloak-proxy:latest
  #   args:
  #     - --resource=uri=/*
  #     - --discovery-url=https://discovery-url
  #     - --client-id=client
  #     - --client-secret=secret
  #     - --listen=0.0.0.0:5602
  #     - --upstream-url=http://127.0.0.1:5601
  #   ports:
  #     - name: web
  #       containerPort: 9090
  resources:
    limits:
      cpu: 100m
      memory: 300Mi
    requests:
      cpu: 100m
      memory: 300Mi

  priorityClassName: ""
  affinity: {}
  tolerations: []
  nodeSelector: {}
  podAnnotations: {}
  replicaCount: 1
  revisionHistoryLimit: 3

  # To export a dashboard from a running Kibana 6.3.x use:
  # curl --user <username>:<password> -XGET https://kibana.yourdomain.com:5601/api/kibana/dashboards/export?dashboard=<some-dashboard-uuid> > my-dashboard.json
  # A dashboard is defined by a name and a string with the json payload or the download url
  dashboardImport:
    timeout: 60
    xpackauth:
      enabled: false
      username: myuser
      password: mypass
    dashboards: {}
      # k8s: https://raw.githubusercontent.com/monotek/kibana-dashboards/master/k8s-fluentd-elasticsearch.json

  # List of pluginns to install using initContainer
  plugins:
    # - https://github.com/sivasamyk/logtrail/releases/download/v0.1.29/logtrail-6.4.0-0.1.29.zip
    # - other_plugin

# helm upgrade --install lexlogger . \
#  --namespace lexlogger \
#  --set global.ingress.annotations."alb\.ingress\.kubernetes\.io\/subnets"="${SUBNETS//,/\\,}" \
#  --set global.ingress.annotations."alb\.ingress\.kubernetes\.io\/security\-groups"="${SECURITY_GROUPS}" \
#  --set kibana.ingress.hosts[0]="kibana\.${DNS_ZONE}" \
#  --values ./values.yaml